{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40588,"status":"ok","timestamp":1697279002640,"user":{"displayName":"Abhiram Acharya","userId":"11880780333368463691"},"user_tz":-330},"id":"YWW1JuRKfRSL","outputId":"62242282-7967-410d-9dcb-2b51244a393c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QXc9B6aLPBJa","executionInfo":{"status":"ok","timestamp":1697279028387,"user_tz":-330,"elapsed":4563,"user":{"displayName":"Abhiram Acharya","userId":"11880780333368463691"}}},"outputs":[],"source":["# Importing Library:\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.xception import Xception\n","from keras.layers import Dense, GlobalAveragePooling2D\n","from keras.models import Model\n","from keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers\n","from keras.layers import Conv2D, MaxPooling2D\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-rT3cSynf--I","executionInfo":{"status":"ok","timestamp":1697279033238,"user_tz":-330,"elapsed":468,"user":{"displayName":"Abhiram Acharya","userId":"11880780333368463691"}}},"outputs":[],"source":["# Dataset Path:\n","BATCH_SIZE = 64\n","IMAGE_SIZE = 300\n","train_path = \"/content/drive/MyDrive/Major Project/Skin disease/train\"\n","test_path = \"/content/drive/MyDrive/Major Project/Skin disease/test\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1YuQvd0Zgg0n","executionInfo":{"status":"ok","timestamp":1697279041524,"user_tz":-330,"elapsed":442,"user":{"displayName":"Abhiram Acharya","userId":"11880780333368463691"}}},"outputs":[],"source":["def train_val_generators(TRAINING_DIR, VALIDATION_DIR,IMAGE_SIZE,BATCH_SIZE):\n","  train_datagen = ImageDataGenerator(rescale=(1./255),\n","                                     shear_range=0.2,\n","                                     zoom_range=0.3,\n","                                     width_shift_range=0.2,\n","                                     height_shift_range=0.2,\n","                                     brightness_range=[0.2,1.2],\n","                                     rotation_range=0.2,\n","                                     horizontal_flip=True)\n","\n","  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n","                                                      batch_size=BATCH_SIZE,\n","                                                      class_mode='categorical',\n","                                                      target_size=(IMAGE_SIZE, IMAGE_SIZE))\n","\n","  test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","\n","  test_generator = test_datagen.flow_from_directory(directory=VALIDATION_DIR,\n","                                                    batch_size=BATCH_SIZE,\n","                                                    class_mode='categorical',\n","                                                    target_size=(IMAGE_SIZE, IMAGE_SIZE))\n","\n","  return train_generator, test_generator"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":496,"status":"error","timestamp":1697279315303,"user":{"displayName":"Abhiram Acharya","userId":"11880780333368463691"},"user_tz":-330},"id":"0V04jG9YQv1S","outputId":"8443ff24-f7bd-4223-fc58-24d723c56b62"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a0f4e6f80d9b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_val_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-903d57529c8e>\u001b[0m in \u001b[0;36mtrain_val_generators\u001b[0;34m(TRAINING_DIR, VALIDATION_DIR, IMAGE_SIZE, BATCH_SIZE)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                      horizontal_flip=True)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                       \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Major Project/Skin disease/train'"]}],"source":["train_generator,test_generator=train_val_generators(train_path, test_path,IMAGE_SIZE,BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PKmkmT5RGMo"},"outputs":[],"source":[" for i in range(5):\n","     img, label = train_generator.next()\n","     print(img.shape)\n","     print(label[0])\n","     plt.imshow(img[0])\n","     plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Meizx4v7RQaa"},"outputs":[],"source":["class_names = train_generator.class_indices\n","class_names"]},{"cell_type":"code","source":[],"metadata":{"id":"Gpo_Qk1aPRkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"em1FXeghTMjR"},"outputs":[],"source":[" from tensorflow.keras.applications.inception_v3 import InceptionV3\n"," base_model = InceptionV3(input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3), include_top = False, weights = 'imagenet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cNaMYx4UGYA"},"outputs":[],"source":[" for layer in base_model.layers:\n","     layer.trainable = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-o5hILFUQtf"},"outputs":[],"source":[" def output_of_last_layer(pre_trained_model, limit_layer):\n","   last_desired_layer = pre_trained_model.get_layer(limit_layer)\n","   print('last layer output shape: ', last_desired_layer.output_shape)\n","   last_output = last_desired_layer.output\n","   print('last layer output: ', last_output)\n","\n","   return last_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ln-QeJHIa4ev"},"outputs":[],"source":["# from keras.models import Sequential\n","# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","\n","# # Create a sequential model\n","# model = Sequential()\n","\n","# # Add the first convolutional layer with 32 filters, a 3x3 kernel, and ReLU activation\n","# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(300,300,3)))\n","\n","# # Add the first pooling layer with 2x2 pool size\n","# model.add(MaxPooling2D((2, 2)))\n","\n","# # Add a second convolutional layer with 64 filters, a 3x3 kernel, and ReLU activation\n","# model.add(Conv2D(64, (3, 3), activation='relu'))\n","\n","# # Add a second pooling layer with 2x2 pool size\n","# model.add(MaxPooling2D((2, 2)))\n","\n","# # Add a third convolutional layer with 128 filters, a 3x3 kernel, and ReLU activation\n","# model.add(Conv2D(128, (3, 3), activation='relu'))\n","\n","# # Add a third pooling layer with 2x2 pool size\n","# model.add(MaxPooling2D((2, 2)))\n","\n","# # Flatten the output of the convolutional layers\n","# model.add(Flatten())\n","\n","# # Add a fully connected layer with 512 neurons and ReLU activation\n","# model.add(Dense(512, activation='relu'))\n","\n","# # Add dropout regularization\n","# model.add(Dropout(0.5))\n","\n","# # Add a final softmax layer for classification\n","# model.add(Dense(23, activation='softmax'))\n","\n","# # Compile the model\n","# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkhsGWfrUa8_"},"outputs":[],"source":["# import tensorflow as tf\n","# from tensorflow.keras.optimizers import Adam\n","# from tensorflow.keras import regularizers\n","# from keras.layers import Conv2D, MaxPooling2D\n","\n","# x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), kernel_regularizer=regularizers.l2(0.01))(last_output)\n","# x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","# x = tf.keras.layers.BatchNormalization()(x)\n","# x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","# # Commenting out the following MaxPooling2D layer\n","# # x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3))(x)\n","# x = tf.keras.layers.Dropout(0.3)(x)\n","# x = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","# x = tf.keras.layers.BatchNormalization()(x)\n","# x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","# x = tf.keras.layers.Dropout(0.3)(x)\n","# x = tf.keras.layers.Dense(23, activation='softmax')(x)\n","\n","# model = tf.keras.models.Model(base_model.input, x)\n","\n","# model.compile(\n","#     optimizer = Adam(learning_rate=0.001),\n","#     loss = 'categorical_crossentropy',\n","#     metrics = ['accuracy']\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bky6OMxBW5oI"},"outputs":[],"source":["base_model = Xception(include_top=False, weights='imagenet', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n","\n","# Freeze the base model layers\n","for layer in base_model.layers:\n","    layer.trainable = True\n","\n","# Add new layers\n","x = base_model.output\n","x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","x = tf.keras.layers.BatchNormalization()(x)\n","x = tf.keras.layers.Dropout(0.3)(x)\n","x = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","x = tf.keras.layers.BatchNormalization()(x)\n","x = tf.keras.layers.Dropout(0.3)(x)\n","x = tf.keras.layers.Dense(23, activation='softmax')(x)\n","\n","model = tf.keras.models.Model(base_model.input, x)\n","\n","model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9g0-hgjU_35"},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","custom_early_stopping = EarlyStopping(\n","    monitor='val_accuracy',\n","    patience=10,\n","    min_delta=0.001,\n","    mode='min'\n",")"]},{"cell_type":"markdown","metadata":{"id":"NKJOSt9gIhRp"},"source":["*Using Xception model*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"BXowRE2zIKfA","outputId":"8aa79911-1568-4ccc-dd0e-4a0788bfd2cb","executionInfo":{"status":"error","timestamp":1691388195473,"user_tz":-330,"elapsed":6807,"user":{"displayName":"Meghan Mane","userId":"02594856939506866077"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n","83683744/83683744 [==============================] - 0s 0us/step\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-afd8c91da3b5>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mtest_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Major Project/Skin disease/train'"]}],"source":["from keras.applications.xception import Xception\n","from keras.layers import Dense, GlobalAveragePooling2D\n","from keras.models import Model\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","# Parameters\n","img_width, img_height = 299, 299\n","train_data_dir = \"/content/drive/MyDrive/Major Project/Skin disease/train\"\n","validation_data_dir = \"/content/drive/MyDrive/Major Project/Skin disease/test\"\n","batch_size = 32\n","epochs = 10\n","num_classes = 23\n","\n","# Load the pre-trained Xception model\n","base_model = Xception(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n","\n","# Freeze the base model layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Add new layers\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","predictions = Dense(num_classes, activation='softmax')(x)\n","\n","# Create the final model\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# Compile the model\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Data augmentation\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True)\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical')\n","\n","# Train the model\n","model.fit(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // batch_size,\n","    epochs=epochs,\n","    validation_data=validation_generator,\n","    validation_steps=validation_generator.samples // batch_size)\n","\n","# Save the model\n","model.save('xception_transfer_learning.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EENvw4pgVSQn"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'r', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend(loc=0)\n","plt.figure(figsize=(6,6))\n","\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend(loc=0)\n","plt.figure(figsize=(6,6))\n","\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjJXCZ3kVX-f"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","\n","test_true=test_generator.classes\n","test_pred_raw = model.predict(test_generator)\n","test_pred = np.argmax(test_pred_raw, axis=1)\n","\n","cm = confusion_matrix(test_true, test_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n","fig, ax = plt.subplots(figsize=(15,15))\n","disp.plot(ax=ax,cmap=plt.cm.Blues)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fopb3CtlVa1f"},"outputs":[],"source":["# !mkdir -p saved_model\n","model.save('my_model')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}